import torch
import torch
from training import IQAPerformance
from IQADataset import read_csv, default_loader, NonOverlappingCropPatches
import os
from matplotlib import pyplot as plt 
from ignite.engine import create_supervised_evaluator
from ignite.metrics.metric import Metric


# Load the TorchScript model
scripted_model = torch.jit.load('C:/Users/win 10/Desktop/optim_CNNIQA/torchscript_model.pt')
scripted_model.eval()

# Input data preparation
im_dir="C:/Users/win 10/Desktop/FR_IQA/IQA-optimization/Destorted_images"
im_names, ground_truth_scores, mos_stds = read_csv('C:/Users/win 10/Desktop/FR_IQA/IQA-optimization/Artif_MOS.csv')

predicted_scores=[]
differences=[]

# Perform inference
evaluator = create_supervised_evaluator(scripted_model,
                                            metrics={'IQA_performance': IQAPerformance()},
                                            device=torch.device('cpu'))
MAE_list=[]
RMSE_list=[]
with torch.no_grad():
    for i in range(len(im_names)):
        im = default_loader(os.path.join(im_dir, im_names[i]))
        patches = NonOverlappingCropPatches(im, 32, 32)
        patch_scores = scripted_model(torch.stack(patches).to(torch.device('cpu')))
        '''input_data = torch.tensor(im)  
        output = scripted_model(input_data)'''
        print("Le score de :" ,im_names[i] , "est :", patch_scores.mean().item())
        '''metrics = evaluator.state.metrics
        SROCC, KROCC, PLCC, RMSE, MAE, OR = metrics[IQAPerformance()]
        MAE_list.append(MAE)
        RMSE_list.append(RMSE)
        print("le MAE :", MAE)'''
        predicted_scores.append(patch_scores.mean().item())

# Assuming 'predicted_scores' and 'ground_truth_scores' are lists of predicted and ground truth IQA scores respectively
#performance_metrics = IQAPerformance()

for pred_score, gt_score in zip(predicted_scores, ground_truth_scores):
    difference = abs(pred_score - gt_score)
    differences.append(difference)
    
    #performance_metrics.update((torch.tensor(pred_score), (torch.tensor(gt_score),)))

# Compute the metrics
#srocc, krocc, plcc, rmse, mae, or_value = performance_metrics.compute()

# Plot the differences and metrics
plt.figure(figsize=(10, 8))

# Plot Differences
plt.subplot(3, 1, 1)
plt.plot(differences, label='Differences', color="blue")
plt.xlabel('im_names')
plt.ylabel('Difference')
plt.legend()

'''# Plot MAE
plt.subplot(3, 1, 2)
plt.plot(MAE, label='MAE', color='red')
plt.xlabel(im_names,label='Image Index')
plt.ylabel('MAE')
plt.legend()

# Plot RMSE
plt.subplot(3, 1, 3)
plt.plot(RMSE, label='RMSE', color='green')
plt.xlabel(im_names,label='Image Index')
plt.ylabel('RMSE')
plt.legend()'''

plt.tight_layout()
plt.show()